{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#add the root directory\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ANN_regression import *\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from loader import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Train ANN for reconstruction of the full state space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Prepare all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_autoignition = \"isochoric-adiabatic-closed-HR-H2-air-lin_Z_0.015_0.035_100-T0_900-\"\n",
    "path_data_autoignition = \"data-files/autoignition/\"\n",
    "path_data = \"data-files/\"\n",
    "\n",
    "scaleManifold = True\n",
    "\n",
    "#create all the datasets\n",
    "mixture_fractions_train = np.loadtxt(f\"{path_data_autoignition}{filename_autoignition}mixture-fraction.csv\") #1 x nbr_timesteps\n",
    "mixture_fractions_test = np.loadtxt(f\"{path_data_autoignition}{filename_autoignition}mixture-fractions-test-trajectories.csv\") #1 x nbr_test_trajectories\n",
    "state_space_names = np.genfromtxt(f\"{path_data_autoignition}{filename_autoignition}state-space-names.csv\", delimiter=\",\", dtype=str)\n",
    "state_space_train = pd.read_csv(f\"{path_data_autoignition}{filename_autoignition}state-space.csv\", names = state_space_names)\n",
    "state_space_source_train = pd.read_csv(f\"{path_data_autoignition}{filename_autoignition}state-space-sources.csv\", names = state_space_names)\n",
    "time_train = np.loadtxt(f\"{path_data_autoignition}{filename_autoignition}time.csv\") #1 x nbr_timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<h3> Create the dataset for the PV of Xu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xu_H2O = \"Xu\"\n",
    "\n",
    "if(Xu_H2O == \"H2O\"):\n",
    "    PV_Xu_train = state_space_train[\"H2O\"]\n",
    "elif(Xu_H2O == \"Xu\"):\n",
    "    PV_Xu_train = state_space_train[\"H2O\"] - state_space_train[\"H2\"] - state_space_train[\"O2\"]\n",
    "PV_Xu_train = PV_Xu_train.to_numpy().reshape(-1,1)\n",
    "\n",
    "if(scaleManifold):\n",
    "    df = pd.DataFrame({\n",
    "        'PV Xu': PV_Xu_train.squeeze(),\n",
    "        'mixture_fraction': mixture_fractions_train\n",
    "    })\n",
    "\n",
    "    # Normalize PV within each mixture_fraction group\n",
    "    df['PV_Xu_scaled'] = df.groupby('mixture_fraction')['PV Xu'].transform(\n",
    "        lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0.0\n",
    "    )\n",
    "\n",
    "    # Get the PV back to numpy\n",
    "    PV_Xu_train = df['PV_Xu_scaled'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "f_PV_Xu_train = np.hstack((mixture_fractions_train.reshape(-1,1), PV_Xu_train))\n",
    "\n",
    "Output_species = ['H2O2', 'H2O', 'H2', 'HO2', 'N2O', 'NO2', 'NO', 'O2', 'OH', 'T']\n",
    "\n",
    "output_matrix = state_space_train[Output_species].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create the dataset for the optimized PV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(241519, 21)\n"
     ]
    }
   ],
   "source": [
    "filename_model = \"Xu-AE-opt_adam-epo_100000-lr_0.025-date_24Mar2025-hour_10h50_Tr0D-2s_s2\" #\"Xu-AE-opt_adam-epo_100000-lr_0.025-date_14Mar2025-hour_14h23_Tr0D-2q_s0\"\n",
    "\n",
    "filename_metadata = filename_model + \"_metadata.pkl\"\n",
    "path_metadata = \"metadata/\"\n",
    "filename_species_names = \"Xu-state-space-names.csv\"\n",
    "path_data = \"data-files/\"\n",
    "\n",
    "loader = loadData(filename_species_names, path_metadata, filename_metadata)\n",
    "idx_species_removed = loader.metadata[\"list idx species removed source\"] if loader.metadata[\"dataset_type\"].startswith(\"autoignition_augm\") else loader.metadata[\"idx species removed\"]\n",
    "id_model = loader.metadata[\"Training_id\"]\n",
    "model = loader.loadModel()\n",
    "\n",
    "weight_inversion = False\n",
    "if(weight_inversion):\n",
    "    with torch.no_grad():  # Ensures we do not track gradients for this operation\n",
    "        model.encoder_species.weight.mul_(-1)\n",
    "\n",
    "state_space_names_DNS = np.genfromtxt(f\"{path_data}Xu-state-space-names.csv\", delimiter=\",\", dtype=str)\n",
    "\n",
    "#create a np array in the format for the DNS dataset/optimized PV\n",
    "state_space_train_DNS = state_space_train[state_space_names_DNS].to_numpy()\n",
    "state_space_source_train_DNS = state_space_source_train[state_space_names_DNS].to_numpy()\n",
    "print(state_space_source_train_DNS.shape)\n",
    "\n",
    "PV_optimized_train = model.get_PV(torch.from_numpy(np.delete(state_space_train_DNS, idx_species_removed, axis=1))).detach().numpy()\n",
    "PV_optimized_min_train = PV_optimized_train.min()\n",
    "PV_optimized_max_train = PV_optimized_train.max()\n",
    "\n",
    "if(scaleManifold):\n",
    "    df = pd.DataFrame({\n",
    "        'PV optimized': PV_optimized_train.squeeze(),\n",
    "        'mixture_fraction': mixture_fractions_train\n",
    "    })\n",
    "\n",
    "    # Normalize PV within each mixture_fraction group\n",
    "    df['PV_optimized_scaled'] = df.groupby('mixture_fraction')['PV optimized'].transform(\n",
    "        lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0.0\n",
    "    )\n",
    "\n",
    "    # Get the PV back to numpy\n",
    "    PV_optimized_train = df['PV_optimized_scaled'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "f_PV_optimized_train = np.hstack((mixture_fractions_train.reshape(-1,1), PV_optimized_train.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Training of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the hyperparameters\n",
    "\n",
    "Xu_optimized = \"Optimized\"   # Chose PV of Xu or optimized PV\n",
    "max_epo = 500         # number of epochs\n",
    "my_seed = 7           # seed for reproducibility\n",
    "std_weights = 0.05    # Standard deviation of the weights at initialization\n",
    "learning_rate = 0.01  # Initial learning rate\n",
    "cosine_alpha = 0.01   # Factor of decrease during cosine decay. cosine_alpha = 0.01 means that the learning rate is divided by 100 at the end of the cosine schedule compared to the initial learning rate\n",
    "cosine_decay_steps = max_epo     # Number of epochs for the cosine scheduler to reduce the learning rate\n",
    "epo_show_loss = 10    # How frequent should the loss be shown in the terminal\n",
    "batch_size = 1000     # Number of datapoints in each batch\n",
    "perc_val = 0.1        # Percentage of the dataset used for validation\n",
    "nbr_input = 2         # Input dimension\n",
    "nbr_output = len(Output_species)       # Output dimension\n",
    "neuron_layers = [11, 21, 21] #[5,10,10]     # Number of additional neurons in each hidden layer = [n_out + x1, n_out + x_2, n_out + x_3...]\n",
    "loss = \"MSE\"          # Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_decay(alpha, epo, tot_epo):\n",
    "    \"\"\"\n",
    "    Cosine decay learning rate. Start at the initial learning rate and ends at initial learning rate times alpha.\n",
    "    After tot_epo, the learning rate is constant and equal to initial learning rate times alpha.\n",
    "    Alpha is the multiplier for the final learning rate.\n",
    "    \"\"\"\n",
    "\n",
    "    myEpo = np.min([epo,tot_epo])\n",
    "\n",
    "    return 0.5*(1-alpha)*(1+np.cos(np.pi*myEpo/tot_epo))+alpha\n",
    "\n",
    "\n",
    "class LogMSELoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-7):\n",
    "        super(LogMSELoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        log_y_pred = torch.log(y_pred + self.epsilon + 1) #+1 to avoid negative values\n",
    "        log_y_true = torch.log(y_true + self.epsilon + 1)\n",
    "        loss = torch.mean((log_y_pred - log_y_true) ** 2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#Initialization of the training\n",
    "###############################\n",
    "\n",
    "epo = 1\n",
    "training_loss_list = np.zeros(max_epo)\n",
    "validation_loss_list = np.zeros(max_epo)\n",
    "best_validation_loss = np.inf\n",
    "epo_best_model = 0\n",
    "\n",
    "\n",
    "#############################\n",
    "#Set seed for reproducibility\n",
    "#############################\n",
    "\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(my_seed)\n",
    "\n",
    "##################################################\n",
    "#Split the data in training and validation dataset\n",
    "#And scale the data and transform to torch tensors\n",
    "##################################################\n",
    "\n",
    "if(Xu_optimized == \"Xu\"):\n",
    "    input_tensor = torch.tensor(f_PV_Xu_train)\n",
    "    output_tensor = torch.tensor(output_matrix)\n",
    "elif(Xu_optimized == \"Optimized\"):\n",
    "    input_tensor = torch.tensor(f_PV_optimized_train)\n",
    "    output_tensor = torch.tensor(output_matrix)\n",
    "\n",
    "#determine number of training and validation samples\n",
    "dataset_size = input_tensor.size(0)\n",
    "train_size = int((1-perc_val) * dataset_size)\n",
    "\n",
    "#shuffle the indices\n",
    "indices = torch.randperm(dataset_size, generator=generator)\n",
    "\n",
    "#split in training and validation indices\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "#create input and output tensors for both training and validation\n",
    "train_input, val_input = input_tensor[train_indices], input_tensor[val_indices]\n",
    "train_output, val_output = output_tensor[train_indices], output_tensor[val_indices]\n",
    "\n",
    "#scale the input features between 0 and 1\n",
    "mins_input = train_input[:, :].min(dim=0, keepdim=True)[0]\n",
    "maxs_input = train_input[:, :].max(dim=0, keepdim=True)[0]\n",
    "input_species_scaling = maxs_input - mins_input\n",
    "\n",
    "#rescale the selected training inputs\n",
    "train_input[:, :] = (train_input[:, :] - mins_input) / input_species_scaling - 0.5\n",
    "\n",
    "#rescale the selected validation inputs\n",
    "val_input[:, :] = (val_input[:, :] - mins_input) / input_species_scaling - 0.5\n",
    "\n",
    "\n",
    "#rescale the output between -1 and 1\n",
    "mins_output = train_output[:, :].min(dim=0, keepdim=True)[0]\n",
    "maxs_output = train_output[:, :].max(dim=0, keepdim=True)[0]\n",
    "\n",
    "#rescale the selected training outputs\n",
    "train_output[:, :] = 2 * (train_output[:, :] - mins_output) / (maxs_output - mins_output) - 1\n",
    "\n",
    "#rescale the selected validation outputs\n",
    "val_output[:, :] = 2 * (val_output[:, :] - mins_output) / (maxs_output - mins_output) - 1\n",
    "\n",
    "\n",
    "nbr_training_datapoints = train_input.size(0)\n",
    "\n",
    "#####################\n",
    "#Initialize the model\n",
    "#####################\n",
    "\n",
    "model = ANN_regression(nbr_input, nbr_output, neuron_layers)\n",
    "model.initialize_model_weights(generator, std_weights)\n",
    "\n",
    "model_params = {\"nbr_input\": nbr_input,\n",
    "                \"nbr_output\": nbr_output,\n",
    "                \"neuron_layers\": neuron_layers}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "if(loss == \"MSE\"):\n",
    "    loss_criterion = nn.MSELoss()\n",
    "elif(loss == \"logMSE\"):\n",
    "    loss_criterion = LogMSELoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR( #cosine decay learning rate scheduler\n",
    "                optimizer, lr_lambda=lambda epoch: cosine_decay(cosine_alpha, epoch, cosine_decay_steps)\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the training of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch: 10 - Training loss (1e-04): 4.6 - Validation loss (1e-04): 3.9\n",
      "Current epoch: 20 - Training loss (1e-04): 2.7 - Validation loss (1e-04): 4.8\n",
      "Current epoch: 30 - Training loss (1e-04): 1.7 - Validation loss (1e-04): 1.4\n",
      "Current epoch: 40 - Training loss (1e-04): 1.8 - Validation loss (1e-04): 0.8\n",
      "Current epoch: 50 - Training loss (1e-04): 1.2 - Validation loss (1e-04): 0.8\n",
      "Current epoch: 60 - Training loss (1e-04): 1.2 - Validation loss (1e-04): 2.8\n",
      "Current epoch: 70 - Training loss (1e-04): 1.0 - Validation loss (1e-04): 0.6\n",
      "Current epoch: 80 - Training loss (1e-04): 1.1 - Validation loss (1e-04): 0.7\n",
      "Current epoch: 90 - Training loss (1e-04): 0.8 - Validation loss (1e-04): 1.0\n",
      "Current epoch: 100 - Training loss (1e-04): 0.8 - Validation loss (1e-04): 0.5\n",
      "Current epoch: 110 - Training loss (1e-04): 0.8 - Validation loss (1e-04): 0.4\n",
      "Current epoch: 120 - Training loss (1e-04): 0.6 - Validation loss (1e-04): 1.1\n",
      "Current epoch: 130 - Training loss (1e-04): 0.7 - Validation loss (1e-04): 0.6\n",
      "Current epoch: 140 - Training loss (1e-04): 0.6 - Validation loss (1e-04): 0.3\n",
      "Current epoch: 150 - Training loss (1e-04): 0.6 - Validation loss (1e-04): 0.3\n",
      "Current epoch: 160 - Training loss (1e-04): 0.7 - Validation loss (1e-04): 0.5\n",
      "Current epoch: 170 - Training loss (1e-04): 0.5 - Validation loss (1e-04): 0.5\n",
      "Current epoch: 180 - Training loss (1e-04): 0.4 - Validation loss (1e-04): 0.5\n",
      "Current epoch: 190 - Training loss (1e-04): 0.4 - Validation loss (1e-04): 0.4\n",
      "Current epoch: 200 - Training loss (1e-04): 0.4 - Validation loss (1e-04): 0.7\n",
      "Current epoch: 210 - Training loss (1e-04): 0.4 - Validation loss (1e-04): 0.4\n",
      "Current epoch: 220 - Training loss (1e-04): 0.3 - Validation loss (1e-04): 0.2\n",
      "Current epoch: 230 - Training loss (1e-04): 0.3 - Validation loss (1e-04): 0.7\n",
      "Current epoch: 240 - Training loss (1e-04): 0.3 - Validation loss (1e-04): 0.5\n",
      "Current epoch: 250 - Training loss (1e-04): 0.3 - Validation loss (1e-04): 0.2\n",
      "Current epoch: 260 - Training loss (1e-04): 0.2 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 270 - Training loss (1e-04): 0.2 - Validation loss (1e-04): 0.2\n",
      "Current epoch: 280 - Training loss (1e-04): 0.2 - Validation loss (1e-04): 0.8\n",
      "Current epoch: 290 - Training loss (1e-04): 0.2 - Validation loss (1e-04): 0.2\n",
      "Current epoch: 300 - Training loss (1e-04): 0.2 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 310 - Training loss (1e-04): 0.2 - Validation loss (1e-04): 0.3\n",
      "Current epoch: 320 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.2\n",
      "Current epoch: 330 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 340 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 350 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 360 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 370 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 380 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 390 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 400 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 410 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 420 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 430 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 440 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 450 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 460 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 470 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 480 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 490 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n",
      "Current epoch: 500 - Training loss (1e-04): 0.1 - Validation loss (1e-04): 0.1\n"
     ]
    }
   ],
   "source": [
    "smallest_training_loss = np.inf\n",
    "smallest_validation_loss = np.inf\n",
    "best_model_state_dict = model.state_dict()\n",
    "epo_best_model = -1\n",
    "\n",
    "while(epo<=max_epo):\n",
    "    #add criterion stop model\n",
    "    \n",
    "    training_loss = 0\n",
    "    validation_loss = 0\n",
    "\n",
    "    #Perform the minibatching\n",
    "    torch.manual_seed(epo) #seed value is the epoch number\n",
    "    indices = torch.randperm(nbr_training_datapoints) #shuffle the indices\n",
    "    split_indices = torch.split(indices, batch_size) #split in subtensors for the batches\n",
    "    \n",
    "    #Start training\n",
    "    for batch_idx in split_indices:\n",
    "\n",
    "        output_model = model(train_input[batch_idx,:])\n",
    "       \n",
    "        #get MSE loss\n",
    "        loss = loss_criterion(output_model, train_output[batch_idx,:])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += len(batch_idx)*loss.detach().cpu().numpy()\n",
    "\n",
    "    #############\n",
    "    #End training\n",
    "    #############\n",
    "\n",
    "    #################\n",
    "    #Begin validation\n",
    "    #################\n",
    "\n",
    "    output_model = model(val_input)\n",
    "\n",
    "    #get MSE loss\n",
    "    loss = loss_criterion(output_model, val_output)\n",
    "\n",
    "    validation_loss += loss.detach().cpu().numpy()\n",
    "    ###############\n",
    "    #End validation\n",
    "    ###############\n",
    "    \n",
    "    #########################\n",
    "    #Post-processing of epoch\n",
    "    #########################\n",
    "\n",
    "    #save training and validation loss\n",
    "    training_loss_list[epo-1] = training_loss/nbr_training_datapoints #weighted average of all the training losses\n",
    "    validation_loss_list[epo-1] = validation_loss\n",
    "    \n",
    "    epo += 1\n",
    "    scheduler.step() #next learning rate in the scheduler\n",
    "\n",
    "    if(epo%epo_show_loss==0):\n",
    "        print(f\"Current epoch: {epo} - Training loss (1e-04): {np.round(training_loss_list[epo-2]*10000,1)} - Validation loss (1e-04): {np.round(validation_loss_list[epo-2]*10000,1)}\")\n",
    "    \n",
    "    # Save model with the smallest validation error\n",
    "    if(validation_loss < smallest_validation_loss):\n",
    "        smallest_training_loss = training_loss/nbr_training_datapoints\n",
    "        smallest_validation_loss = validation_loss\n",
    "        best_model_state_dict = model.state_dict()\n",
    "        epo_best_model = epo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model and its hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the data\n",
    "metadata = {\"data\": Xu_optimized, \"model_params\": model_params, \"autoignition data\": filename_autoignition, \"final training error\": smallest_training_loss,\n",
    "            \"final validation error\": smallest_validation_loss, \"max_epo\": max_epo,\n",
    "            \"my_seed\": my_seed,\n",
    "            \"std_weights\": std_weights,\n",
    "            \"learning_rate\":learning_rate,\n",
    "            \"cosine_alpha\": cosine_alpha,\n",
    "            \"cosine_decay_steps\": cosine_decay_steps,\n",
    "            \"epo_show_loss\": epo_show_loss,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"perc_val\": perc_val,\n",
    "            \"nbr_input\": nbr_input,\n",
    "            \"nbr_output\": nbr_output,\n",
    "            \"neuron_layers\": neuron_layers,\n",
    "            \"mins_input\": mins_input,\n",
    "            \"maxs_input\": maxs_input,\n",
    "            \"mins_output\": mins_output,\n",
    "            \"maxs_output\": maxs_output,\n",
    "            \"loss_criterion\": loss,\n",
    "            \"Output_species\": Output_species,\n",
    "            \"Xu_H2O\": Xu_H2O,\n",
    "            \"scaleManifold\": scaleManifold}\n",
    "\n",
    "now = datetime.now()\n",
    "date_str = now.strftime(\"%Y-%m-%d\")\n",
    "time_str = now.strftime(\"%Hh%M\")\n",
    "\n",
    "if(Xu_optimized == \"Xu\"):\n",
    "    with open(f'autoignition/models/{\"Xu\" if Xu_H2O == \"Xu\" else \"H2O\"}_full_{date_str}_{time_str}_metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(metadata, f)  \n",
    "elif(Xu_optimized == \"Optimized\"):\n",
    "    with open(f'autoignition/models/Optimized_full_{id_model}_{date_str}_{time_str}_metadata.pkl', 'wb') as f:\n",
    "                pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized_full_Tr0D-2s_s2_2026-01-26_16h02_model.pth\n"
     ]
    }
   ],
   "source": [
    "if(Xu_optimized == \"Xu\"):\n",
    "    torch.save(best_model_state_dict, '{}{}'.format(\"autoignition/models/\", f'{\"Xu\" if Xu_H2O == \"Xu\" else \"H2O\"}_full_{date_str}_{time_str}_model.pth'))\n",
    "    print(f'{\"Xu\" if Xu_H2O == \"Xu\" else \"H2O\"}_full_{date_str}_{time_str}_model.pth')\n",
    "elif(Xu_optimized == \"Optimized\"):\n",
    "    torch.save(best_model_state_dict, '{}{}'.format(\"autoignition/models/\", f'Optimized_full_{id_model}_{date_str}_{time_str}_model.pth'))\n",
    "    print(f'Optimized_full_{id_model}_{date_str}_{time_str}_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvPV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
